apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: platform-metrics-collector
    app.kubernetes.io/part-of: multicluster-observability-addon
    app.kubernetes.io/managed-by: multicluster-observability-operator
  name: platform-rules-default
  namespace: open-cluster-management-observability
spec:
  groups:
  - name: acm-platform-default-rules
    rules:
    - expr: sum(node_memory_MemAvailable_bytes{job="node-exporter"} or (node_memory_Buffers_bytes{job="node-exporter"}
        + node_memory_Cached_bytes{job="node-exporter"} + node_memory_MemFree_bytes{job="node-exporter"}
        + node_memory_Slab_bytes{job="node-exporter"}))
      record: :node_memory_MemAvailable_bytes:sum
    - expr: sum(grpc_server_started_total{job="etcd",grpc_service="etcdserverpb.Lease",grpc_type="bidi_stream"})
        - sum(grpc_server_handled_total{job="etcd",grpc_service="etcdserverpb.Lease",grpc_type="bidi_stream"})
      record: active_streams_lease:grpc_server_handled_total:sum
    - expr: sum(grpc_server_started_total{job="etcd",grpc_service="etcdserverpb.Watch",grpc_type="bidi_stream"})
        - sum(grpc_server_handled_total{job="etcd",grpc_service="etcdserverpb.Watch",grpc_type="bidi_stream"})
      record: active_streams_watch:grpc_server_handled_total:sum
    - expr: (histogram_quantile(0.99,sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",
        verb!="WATCH"}[5m])) by (le)))
      record: apiserver_request_duration_seconds:histogram_quantile_99
    - expr: (histogram_quantile(0.99,sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",
        verb!="WATCH"}[5m])) by (le, verb, instance)))
      record: apiserver_request_duration_seconds:histogram_quantile_99:instance
    - expr: sum(machine_memory_bytes)
      record: cluster:machine_memory:sum
    - expr: sum(sum(sum(kube_pod_container_resource_requests{resource="cpu",unit="core"}) by (pod,namespace,container)
        * on(pod,namespace) group_left(phase) max(kube_pod_status_phase{phase=~"Running|Pending|Unknown"}
        >0) by (pod,namespace,phase)) by (pod,namespace,phase))
      record: cluster:kube_pod_container_resource_requests:cpu:sum
    - expr: sum(sum(sum(kube_pod_container_resource_requests{resource="memory",unit="byte"}) by (pod,namespace,container)
        * on(pod,namespace) group_left(phase) max(kube_pod_status_phase{phase=~"Running|Pending|Unknown"}
        >0) by (pod,namespace,phase)) by (pod,namespace,phase))
      record: cluster:kube_pod_container_resource_requests:memory:sum
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m])) / count(sum(node_cpu_seconds_total)
        BY (instance, cpu))
      record: cluster:node_cpu:ratio  
    - expr: |
        1 - avg without (cpu, mode) (
          rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
        )
      record: instance:node_cpu_utilisation:rate1m
    - expr: |
        (
          node_load1{job="node-exporter"}
        /
          instance:node_num_cpu:sum{job="node-exporter"}
        )
      record: instance:node_load1_per_cpu:ratio
    - expr: |
        1 - (
          node_memory_MemAvailable_bytes{job="node-exporter"}
        /
          node_memory_MemTotal_bytes{job="node-exporter"}
        )
      record: instance:node_memory_utilisation:ratio
    - expr: |
        sum without (device) (
          rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_drop_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_drop_excluding_lo:rate1m
    - expr: |
        count without (cpu) (
          count without (mode) (
            node_cpu_seconds_total{job="node-exporter"}
          )
        )
      record: instance:node_num_cpu:sum
    - expr: |
        rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
      record: instance:node_vmstat_pgmajfault:rate1m
    - expr: |
        rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_seconds:rate1m
    - expr: |
        rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_weighted_seconds:rate1m
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
              "replicaset", "$1", "owner_name", "(.*)"
            ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
              1, max by (replicaset, namespace, owner_name) (
                kube_replicaset_owner{job="kube-state-metrics"}
              )
            ),
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: deployment
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: daemonset
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: statefulset
      record: namespace_workload_pod:kube_pod_owner:relabel
    - record: namespace_workload_pod:kube_pod_owner:relabel:avg
      expr: count(avg(namespace_workload_pod:kube_pod_owner:relabel{pod!=""}) by (workload, namespace)) by (namespace)
    - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum
      expr: sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{container!=""}) by (namespace) or sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate{container!=""}) by (namespace)
    - expr: sum(rate(grpc_server_handled_total{job="etcd",grpc_type="unary",grpc_code!="OK"}[5m]))
      record: rpc_rate:grpc_server_handled_total:sum_rate
    - expr: sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",service="kubernetes",le="1",verb=~"POST|PUT|DELETE|PATCH"}[1m]))
        / sum(increase(apiserver_request_duration_seconds_bucket{job="apiserver",service="kubernetes",verb=~"POST|PUT|DELETE|PATCH"}[1m]))
      record: sli:apiserver_request_duration_seconds:trend:1m
    - expr: sli:apiserver_request_duration_seconds:trend:1m >= bool 0.9900
      record: sli:apiserver_request_duration_seconds:bin:trend:1m
    - expr: sum(rate(apiserver_request_total{job="apiserver"}[1h])) by (code, instance)
      record: sum:apiserver_request_total:1h
    - expr: sum(rate(apiserver_request_total{job="apiserver"}[5m])) by (code, instance)
      record: sum:apiserver_request_total:5m
    - record: kube_pod_container_resource_limits:sum
      expr: sum(kube_pod_container_resource_limits) by (resource, namespace)
    - expr: |-
        sum by (cluster, namespace, pod, container) (
          rate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
        ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
          1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
    - expr: histogram_quantile(0.99, sum(rate(workqueue_queue_duration_seconds_bucket{job="apiserver"}[5m])) by (instance, name, le))
      record: workqueue_queue_duration_seconds_bucket:apiserver:histogram_quantile_99
    - expr: sum(rate(grpc_server_started_total{job="etcd",grpc_type="unary"}[5m]))
      record: grpc_server_started_total:etcd_unary:sum_rate
    - expr: avg(rate(node_cpu_seconds_total{mode="idle"}[5m]))
      record: node_cpu_seconds_total:mode_idle:avg_rate5m
    - expr: sum(cluster:kube_pod_container_resource_requests:memory:sum) by (cluster) / sum(kube_node_status_allocatable{resource="memory"}) by (cluster)
      record: cluster:memory_requested:ratio
    - expr: 1 - sum(:node_memory_MemAvailable_bytes:sum) by (cluster) / sum(kube_node_status_allocatable{resource="memory"}) by (cluster)
      record: cluster:memory_utilized:ratio
    - expr: sum(kube_node_status_allocatable{resource="cpu"}) by (cluster)
      record: cluster:cpu_allocatable:sum
    - expr: sum(cluster:kube_pod_container_resource_requests:cpu:sum) by (cluster) / sum(kube_node_status_allocatable{resource="cpu"}) by (cluster)
      record: cluster:cpu_requested:ratio
    - expr: sum(machine_cpu_cores) by (cluster)
      record: cluster:cpu_cores:sum
    - record: namespace_pod:container_network_receive_bytes_total:sum
      expr: sum(irate(container_network_receive_bytes_total[5m])) by (namespace, pod)
    - record: namespace_pod:container_network_receive_packets_total:sum
      expr: sum(irate(container_network_receive_packets_total[5m])) by (namespace, pod)
    - record: namespace_pod:container_network_receive_packets_dropped_total:sum
      expr: sum(irate(container_network_receive_packets_dropped_total[5m])) by (namespace, pod)
    - record: namespace_pod:container_network_transmit_packets_total:sum
      expr: sum(irate(container_network_transmit_packets_total[5m])) by (namespace, pod)
    - record: namespace_pod:container_network_transmit_bytes_total:sum
      expr: sum(irate(container_network_transmit_bytes_total[5m])) by (namespace, pod)
    - record: namespace_pod:container_network_transmit_packets_dropped_total:sum
      expr: sum(irate(container_network_transmit_packets_dropped_total[5m])) by (namespace, pod)
  - name: kubernetes-storage
    rules:
      - alert: KubePersistentVolumeFillingUp1Min
        annotations:
          summary: PersistentVolume is filling up.
          description: "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
        expr: kubelet_volume_stats_available_bytes{namespace="open-cluster-management-observability"}/kubelet_volume_stats_capacity_bytes{namespace="open-cluster-management-observability"} < 0.03
        for: 1m
        labels:
          instance: "{{ $labels.instance }}"
          cluster: "{{ $labels.cluster }}"
          clusterID: "{{ $labels.clusterID }}"
          PersistentVolumeClaim: "{{ $labels.persistentvolumeclaim }}"
          severity: info
      - alert: KubePersistentVolumeFillingUp1Hour
        annotations:
          summary: PersistentVolume is filling up and is predicted to run out of space in 6h.
          description: "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
        expr: (kubelet_volume_stats_available_bytes{namespace="open-cluster-management-observability"}/kubelet_volume_stats_capacity_bytes{namespace="open-cluster-management-observability"}) < 0.15 and (predict_linear(kubelet_volume_stats_available_bytes{namespace="open-cluster-management-observability"}[6h], 4 * 24 * 3600)) <0
        for: 1h
        labels:
          instance: "{{ $labels.instance }}"
          cluster: "{{ $labels.cluster }}"
          clusterID: "{{ $labels.clusterID }}"
          PersistentVolumeClaim: "{{ $labels.persistentvolumeclaim }}"
          severity: info
  - name: policy-reports
    rules:
      - alert: ViolatedPolicyReport
        annotations:
          summary: "There is a policy report violation with a {{ $labels.severity }} severity level detected."
          description: "The policy: {{ $labels.policy }} has a severity of {{ $labels.severity }} on cluster: {{ $labels.cluster }}"
        expr: sum without (managed_cluster_id) (label_replace((sum without (clusterID) (label_replace((sum(policyreport_info * on (managed_cluster_id) group_left (cluster) label_replace(acm_managed_cluster_labels, "cluster", "$1", "name", "(.*)")) by (cluster, category, clusterID, managed_cluster_id, policy, severity) > 0),"hub_cluster_id", "$1", "clusterID", "(.*)"))),"clusterID", "$1", "managed_cluster_id", "(.*)"))
        for: 1m
        labels:
          severity: "{{ $labels.severity }}"
  - name: acm-metrics-collector-federation-alerts
    rules:
      - alert: HighFederationEndpointErrorRate5Percent
        expr: |
          (
            sum(
              rate(prometheus_http_requests_total{handler="/federate",namespace=~"openshift-monitoring|openshift-user-workload-monitoring",job=~"prometheus-k8s|prometheus-user-workload",code=~"5..|4.."}[5m])
            ) by (job, pod) 
            / 
            sum(
              rate(prometheus_http_requests_total{handler="/federate",namespace=~"openshift-monitoring|openshift-user-workload-monitoring",job=~"prometheus-k8s|prometheus-user-workload"}[5m])
            ) by (job, pod)
            * 100
          ) > 5
          AND 
          sum(
            rate(prometheus_http_requests_total{handler="/federate",namespace=~"openshift-monitoring|openshift-user-workload-monitoring",job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          ) by (job, pod) 
          > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: High error rate on /federate endpoint for {{ $labels.job }}/{{ $labels.pod }}.
          description: 'The /federate endpoint on pod {{ $labels.pod }} (job {{ $labels.job }}) is experiencing an error rate of {{ printf "%.2f" $value }}% over the last 10 minutes, which is above the 5% threshold. This could be caused by Prometheus being overloaded, network issues, or misconfiguration. Check the logs of the pod.'
      - alert: HighFederationEndpointLatencyOver8s
        expr: |
          sum (
            increase(prometheus_http_request_duration_seconds_bucket{handler="/federate",namespace=~"openshift-monitoring|openshift-user-workload-monitoring",job=~"prometheus-k8s|prometheus-user-workload", le="+Inf"}[10m])
            - 
            increase(prometheus_http_request_duration_seconds_bucket{handler="/federate",namespace=~"openshift-monitoring|openshift-user-workload-monitoring",job=~"prometheus-k8s|prometheus-user-workload", le="8"}[10m])
          ) by (job, pod) 
          >0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: High latency on /federate endpoint ({{ $labels.job }}/{{ $labels.pod }}).
          description: '{{ printf "%.0f" $value }} federation request(s) on pod {{ $labels.pod }} (job {{ $labels.job }}) took longer than 8 seconds in the last 10 minutes. This indicates potential performance degradation or a scrape query federating too many metrics. Check for high CPU/memory usage or long-running queries on the Prometheus pod.'
      - alert: HighFederationEndpointLatencyOver20s
        expr: |
          sum (
            increase(prometheus_http_request_duration_seconds_bucket{handler="/federate",namespace=~"openshift-monitoring|openshift-user-workload-monitoring",job=~"prometheus-k8s|prometheus-user-workload", le="+Inf"}[10m])
            - 
            increase(prometheus_http_request_duration_seconds_bucket{handler="/federate",namespace=~"openshift-monitoring|openshift-user-workload-monitoring",job=~"prometheus-k8s|prometheus-user-workload", le="20"}[10m])
          ) by (job, pod) 
          > 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: Critically high latency on /federate endpoint ({{ $labels.job }}/{{ $labels.pod }}).
          description: '{{ printf "%.0f" $value }} federation request(s) on pod {{ $labels.pod }} (job {{ $labels.job }}) took longer than 20 seconds in the last 10 minutes. This is critically close to the 30s proxy timeout and may lead to failed scrapes. Investigate performance on the Prometheus pod immediately.'
      - alert: MetricsCollectorNotIngestingSamples
        expr: rate(prometheus_agent_samples_appended_total{job=~"platform-metrics-collector|user-workload-metrics-collector"}[5m]) == 0
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: Prometheus Agent {{ $labels.namespace }}/{{ $labels.pod }} is not ingesting any sample.
          description: 'The Prometheus Agent pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has reported an ingestion rate of 0 samples/sec. This indicates a total stall. Check the logs of the pod for errors and verify its state.'
  - name: acm-metrics-collector-remote-write-alerts
    rules:
      - alert: MetricsCollectorRemoteWriteFailures
        expr: |
          rate(prometheus_remote_storage_samples_failed_total{job=~"platform-metrics-collector|user-workload-metrics-collector",remote_name="acm-observability"}[5m]) 
          / 
          (
            rate(prometheus_remote_storage_samples_failed_total{job=~"platform-metrics-collector|user-workload-metrics-collector",remote_name="acm-observability"}[5m] )
            + 
            rate(prometheus_remote_storage_samples_total{job=~"platform-metrics-collector|user-workload-metrics-collector",remote_name="acm-observability"}[5m])
          ) 
          * 100 
          > 1
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: Prometheus Agent {{ $labels.namespace }}/{{ $labels.pod }} fails to remote write to the Hub.
          description: 'The Prometheus Agent pod {{ $labels.pod }} in namespace {{ $labels.namespace }} fails to send {{ printf "%.1f" $value }}% of its samples to remote_name {{ $labels.remote_name }}. Check Agent logs and Hub receiver.'
      - alert: MetricsCollectorRemoteWriteBehind
        expr: |
          (
            max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=~"platform-metrics-collector|user-workload-metrics-collector"}[5m]) 
            - 
            ignoring(remote_name, url) group_right max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~"platform-metrics-collector|user-workload-metrics-collector"}[5m])
          ) > 120
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Prometheus Agent {{ $labels.namespace }}/{{ $labels.pod }} remote write to the Hub is behind.
          description: 'The Prometheus Agent pod {{ $labels.pod }} (namespace {{ $labels.namespace }}) remote write is {{ printf "%.0f" $value }}s behind for remote name {{ $labels.remote_name}}. This may indicate network latency or a struggling Hub receiver.'
      - alert: MetricsCollectorRemoteWriteDesiredShards
        expr: |
          max_over_time(prometheus_remote_storage_shards_desired{job=~"platform-metrics-collector|user-workload-metrics-collector"}[5m]) 
          > 
          max_over_time(prometheus_remote_storage_shards_max{job=~"platform-metrics-collector|user-workload-metrics-collector"}[5m])
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Prometheus Agent {{ $labels.namespace }}/{{ $labels.pod }} remote write needs more shards.
          description: 'The Prometheus Agent pod {{ $labels.pod }} (namespace {{ $labels.namespace }}) remote write wants to run {{ printf "%.1f" $value }} shards for remote_name {{ $labels.remote_name}}, exceeding its configured maximum. This indicates sustained backpressure. Chack the queueConfig.maxShards for this remote_name or use the metric prometheus_remote_storage_shards_max.'
