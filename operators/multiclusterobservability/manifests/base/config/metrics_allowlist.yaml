kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-allowlist
data:
  metrics_list.yaml: |
    names:
      - :node_memory_MemAvailable_bytes:sum
      - ALERTS
      - acm_managed_cluster_labels
      - authenticated_user_requests
      - authentication_attempts
      - cluster:capacity_cpu_cores:sum
      - cluster:capacity_memory_bytes:sum
      - cluster:container_cpu_usage:ratio
      - cluster:container_spec_cpu_shares:ratio
      - cluster:cpu_usage_cores:sum
      - cluster:memory_usage:ratio
      - cluster:memory_usage_bytes:sum
      - cluster:usage:resources:sum
      - cluster_infrastructure_provider
      - cluster_version
      - cluster_version_payload
      - container_cpu_cfs_periods_total
      - container_cpu_cfs_throttled_periods_total
      - container_spec_cpu_quota
      - coredns_dns_request_count_total
      - coredns_dns_request_duration_seconds_sum
      - coredns_dns_request_type_count_total
      - coredns_dns_response_rcode_count_total
      - etcd_debugging_mvcc_db_total_size_in_bytes
      - etcd_mvcc_db_total_size_in_bytes
      - etcd_debugging_snap_save_total_duration_seconds_sum
      - etcd_disk_backend_commit_duration_seconds_bucket
      - etcd_disk_backend_commit_duration_seconds_sum
      - etcd_disk_wal_fsync_duration_seconds_bucket
      - etcd_disk_wal_fsync_duration_seconds_sum
      - etcd_object_counts
      - etcd_network_client_grpc_received_bytes_total
      - etcd_network_client_grpc_sent_bytes_total
      - etcd_network_peer_received_bytes_total
      - etcd_network_peer_sent_bytes_total
      - etcd_server_client_requests_total
      - etcd_server_has_leader
      - etcd_server_health_failures
      - etcd_server_leader_changes_seen_total
      - etcd_server_proposals_failed_total
      - etcd_server_proposals_pending
      - etcd_server_proposals_committed_total
      - etcd_server_proposals_applied_total
      - etcd_server_quota_backend_bytes
      - grpc_server_started_total
      - haproxy_backend_connection_errors_total
      - haproxy_backend_connections_total
      - haproxy_backend_current_queue
      - haproxy_backend_http_average_response_latency_milliseconds
      - haproxy_backend_max_sessions
      - haproxy_backend_response_errors_total
      - haproxy_backend_up
      - http_requests_total
      - instance:node_filesystem_usage:sum
      - instance:node_cpu_utilisation:rate1m
      - instance:node_load1_per_cpu:ratio
      - instance:node_memory_utilisation:ratio
      - instance:node_network_receive_bytes_excluding_lo:rate1m
      - instance:node_network_receive_drop_excluding_lo:rate1m
      - instance:node_network_transmit_bytes_excluding_lo:rate1m
      - instance:node_network_transmit_drop_excluding_lo:rate1m
      - instance:node_num_cpu:sum
      - instance:node_vmstat_pgmajfault:rate1m
      - instance_device:node_disk_io_time_seconds:rate1m
      - instance_device:node_disk_io_time_weighted_seconds:rate1m
      - kube_daemonset_status_desired_number_scheduled
      - kube_daemonset_status_number_unavailable
      - kube_node_spec_unschedulable
      - kube_node_status_allocatable
      - kube_node_status_allocatable_cpu_cores
      - kube_node_status_allocatable_memory_bytes
      - kube_node_status_capacity
      - kube_node_status_capacity_pods
      - kube_node_status_capacity_cpu_cores
      - kube_node_status_condition
      - kube_pod_container_resource_limits
      - kube_pod_container_resource_limits_cpu_cores
      - kube_pod_container_resource_limits_memory_bytes
      - kube_pod_container_resource_requests
      - kube_pod_container_resource_requests_cpu_cores
      - kube_pod_container_resource_requests_memory_bytes
      - kube_pod_info
      - kube_pod_owner
      - kube_resourcequota
      - kubelet_running_container_count
      - kubelet_runtime_operations
      - kubelet_runtime_operations_latency_microseconds
      - kubelet_volume_stats_available_bytes
      - kubelet_volume_stats_capacity_bytes
      - kube_persistentvolume_status_phase
      - machine_cpu_cores
      - machine_memory_bytes
      - mixin_pod_workload
      - namespace:kube_pod_container_resource_requests_cpu_cores:sum
      - namespace:kube_pod_container_resource_requests_memory_bytes:sum
      - namespace:container_memory_usage_bytes:sum
      - namespace_cpu:kube_pod_container_resource_requests:sum
      - namespace_workload_pod:kube_pod_owner:relabel
      - node_cpu_seconds_total
      - node_filesystem_avail_bytes
      - node_filesystem_free_bytes
      - node_filesystem_size_bytes
      - node_memory_MemAvailable_bytes
      - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
      - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
      - node_netstat_Tcp_OutSegs
      - node_netstat_Tcp_RetransSegs
      - node_netstat_TcpExt_TCPSynRetrans
      - policyreport_info
      - up
      - cluster_monitoring_operator_reconcile_errors_total
      - cluster_monitoring_operator_reconcile_attempts_total
      - cluster_operator_conditions
      - cluster_operator_up      
      - cluster:policy_governance_info:propagated_count
      - cluster:policy_governance_info:propagated_noncompliant_count
      - policy:policy_governance_info:propagated_count
      - policy:policy_governance_info:propagated_noncompliant_count
    matches:
      - __name__="workqueue_queue_duration_seconds_bucket",job="apiserver"
      - __name__="workqueue_adds_total",job="apiserver"
      - __name__="workqueue_depth",job="apiserver"
      - __name__="go_goroutines",job="apiserver"
      - __name__="process_cpu_seconds_total",job="apiserver"
      - __name__="process_resident_memory_bytes",job="apiserver"
      - __name__="container_memory_cache",container!=""
      - __name__="container_memory_rss",container!=""
      - __name__="container_memory_swap",container!=""
      - __name__="container_memory_working_set_bytes",container!=""
    renames:
      mixin_pod_workload: namespace_workload_pod:kube_pod_owner:relabel
      namespace:kube_pod_container_resource_requests_cpu_cores:sum: namespace_cpu:kube_pod_container_resource_requests:sum
      node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
      etcd_mvcc_db_total_size_in_bytes: etcd_debugging_mvcc_db_total_size_in_bytes
    recording_rules:
      - record: apiserver_request_duration_seconds:histogram_quantile_99
        expr: histogram_quantile(0.99,sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\", verb!=\"WATCH\"}[5m])) by (le))
      - record: apiserver_request_duration_seconds:histogram_quantile_99:instance
        expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\", verb!=\"WATCH\"}[5m])) by (le, verb, instance))
      - record: sum:apiserver_request_total:1h
        expr: sum(rate(apiserver_request_total{job=\"apiserver\"}[1h])) by(code, instance)
      - record: sum:apiserver_request_total:5m
        expr: sum(rate(apiserver_request_total{job=\"apiserver\"}[5m])) by(code, instance)
      - record: rpc_rate:grpc_server_handled_total:sum_rate
        expr: sum(rate(grpc_server_handled_total{job=\"etcd\",grpc_type=\"unary\",grpc_code!=\"OK\"}[5m]))
      - record: active_streams_watch:grpc_server_handled_total:sum
        expr: sum(grpc_server_started_total{job=\"etcd\",grpc_service=\"etcdserverpb.Watch\",grpc_type=\"bidi_stream\"}) - sum(grpc_server_handled_total{job=\"etcd\",grpc_service=\"etcdserverpb.Watch\",grpc_type=\"bidi_stream\"})
      - record: active_streams_lease:grpc_server_handled_total:sum
        expr: sum(grpc_server_started_total{job=\"etcd\",grpc_service=\"etcdserverpb.Lease\",grpc_type=\"bidi_stream\"}) - sum(grpc_server_handled_total{job=\"etcd\",grpc_service=\"etcdserverpb.Lease\",grpc_type=\"bidi_stream\"})
      - record: cluster:kube_pod_container_resource_requests:cpu:sum
        expr: sum(sum(sum(kube_pod_container_resource_requests{resource=\"cpu\"}) by (pod,namespace,container) * on(pod,namespace) group_left(phase) max(kube_pod_status_phase{phase=~\"Running|Pending|Unknown\"} >0) by (pod,namespace,phase)) by (pod,namespace,phase))
      - record: cluster:kube_pod_container_resource_requests:memory:sum
        expr: sum(sum(sum(kube_pod_container_resource_requests{resource=\"memory\"}) by (pod,namespace,container) * on(pod,namespace) group_left(phase) max(kube_pod_status_phase{phase=~\"Running|Pending|Unknown\"} >0) by (pod,namespace,phase)) by (pod,namespace,phase))
      - record: sli:apiserver_request_duration_seconds:trend:1m
        expr: sum(increase(apiserver_request_duration_seconds_bucket{job=\"apiserver\",service=\"kubernetes\",le=\"1\",verb=~\"POST|PUT|DELETE|PATCH\"}[1m])) / sum(increase(apiserver_request_duration_seconds_count{job=\"apiserver\",service=\"kubernetes\",verb=~\"POST|PUT|DELETE|PATCH\"}[1m]))
      - record: container_memory_rss:sum
        expr: sum(container_memory_rss) by (container, namespace)
      - record: kube_pod_container_resource_limits:sum
        expr: sum(kube_pod_container_resource_limits) by (resource, namespace)
      - record: kube_pod_container_resource_requests:sum
        expr: sum(kube_pod_container_resource_requests{container!=\"\"}) by (resource, namespace)
      - record: namespace_workload_pod:kube_pod_owner:relabel:avg
        expr: count(avg(namespace_workload_pod:kube_pod_owner:relabel{pod!=\"\"}) by (workload, namespace)) by (namespace)
      - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum
        expr: sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{container!=\"\"}) by (namespace) or sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate{container!=\"\"}) by (namespace)
    collect_rules:
      - group: SNOResourceUsage
        annotations:
          description: >
            By default, a SNO cluster does not collect pod and container resource metrics. Once a SNO cluster 
            reaches a level of resource consumption, these granular metrics are collected dynamically. 
            When the cluster resource consumption is consistently less than the threshold for a period of time, 
            collection of the granular metrics stops.
        selector:
          matchExpressions:
            - key: clusterType
              operator: In
              values: ["SNO"]
        rules:
        - collect: SNOHighCPUUsage
          annotations:
            description: >
              Collects the dynamic metrics specified if the cluster cpu usage is constantly more than 70% for 2 minutes
          expr: (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))) * 100 > 70
          for: 2m
          dynamic_metrics:
            names:
              - container_cpu_cfs_periods_total
              - container_cpu_cfs_throttled_periods_total
              - kube_pod_container_resource_limits 
              - kube_pod_container_resource_requests   
              - namespace_workload_pod:kube_pod_owner:relabel 
              - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate 
              - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate 
        - collect: SNOHighMemoryUsage
          annotations:
            description: >
              Collects the dynamic metrics specified if the cluster memory usage is constantly more than 70% for 2 minutes
          expr: (1 - sum(:node_memory_MemAvailable_bytes:sum) / sum(kube_node_status_allocatable{resource=\"memory\"})) * 100 > 70
          for: 2m
          dynamic_metrics:
            names:
              - kube_pod_container_resource_limits 
              - kube_pod_container_resource_requests 
              - namespace_workload_pod:kube_pod_owner:relabel
            matches:
              - __name__="container_memory_cache",container!=""
              - __name__="container_memory_rss",container!=""
              - __name__="container_memory_swap",container!=""
              - __name__="container_memory_working_set_bytes",container!=""
 
  ocp311_metrics_list.yaml: |
    names:
      - :node_memory_MemAvailable_bytes:sum
      - container_cpu_cfs_periods_total
      - container_cpu_cfs_throttled_periods_total
      - etcd_debugging_mvcc_db_total_size_in_bytes
      - etcd_disk_backend_commit_duration_seconds_bucket
      - etcd_disk_wal_fsync_duration_seconds_bucket
      - etcd_network_client_grpc_received_bytes_total
      - etcd_network_client_grpc_sent_bytes_total
      - etcd_network_peer_received_bytes_total
      - etcd_network_peer_sent_bytes_total
      - etcd_server_has_leader
      - etcd_server_leader_changes_seen_total
      - etcd_server_proposals_failed_total
      - etcd_server_proposals_pending
      - etcd_server_proposals_committed_total
      - etcd_server_proposals_applied_total
      - grpc_server_started_total
      - instance:node_cpu_utilisation:rate1m
      - instance:node_load1_per_cpu:ratio
      - instance:node_memory_utilisation:ratio
      - instance:node_network_receive_bytes_excluding_lo:rate1m
      - instance:node_network_receive_drop_excluding_lo:rate1m
      - instance:node_network_transmit_bytes_excluding_lo:rate1m
      - instance:node_network_transmit_drop_excluding_lo:rate1m
      - instance:node_num_cpu:sum
      - instance:node_vmstat_pgmajfault:rate1m
      - instance_device:node_disk_io_time_seconds:rate1m
      - instance_device:node_disk_io_time_weighted_seconds:rate1m
      - kube_node_status_allocatable
      - kube_node_status_allocatable_cpu_cores
      - kube_node_status_allocatable_memory_bytes
      - kube_node_status_capacity_cpu_cores
      - kube_node_status_condition
      - kube_pod_container_resource_limits
      - kube_pod_container_resource_limits_cpu_cores
      - kube_pod_container_resource_limits_memory_bytes
      - kube_pod_container_resource_requests
      - kube_pod_container_resource_requests_cpu_cores
      - kube_pod_container_resource_requests_memory_bytes
      - kube_pod_info
      - kube_pod_owner
      - kube_resourcequota
      - machine_cpu_cores
      - machine_memory_bytes
      - mixin_pod_workload
      - namespace_workload_pod:kube_pod_owner:relabel
      - node_cpu_seconds_total
      - node_filesystem_avail_bytes
      - node_filesystem_size_bytes
      - node_memory_MemAvailable_bytes
      - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
      - node_netstat_Tcp_OutSegs
      - node_netstat_Tcp_RetransSegs
      - node_netstat_TcpExt_TCPSynRetrans
      - up
      - node:node_cpu_utilisation:avg1m
      - kube_node_labels
      - 'node_namespace_pod:kube_pod_info:'
      - container_memory_usage_bytes
      - node_memory_MemTotal_bytes
      - node:node_memory_bytes_total:sum
      - node:node_net_utilisation:sum_irate
      - node_network_receive_bytes
      - node_network_transmit_bytes
      - node_disk_bytes_read
      - node_disk_bytes_written
      - node:node_disk_utilisation:avg_irate
      - kube_pod_status_ready
      - kube_pod_status_phase
      - node_filesystem_size
      - node_filesystem_avail
      - kube_pod_container_status_restarts_total
      - openshift_clusterresourcequota_usage
      - openshift_clusterresourcequota_labels
      - namespace_pod_name_container_name:container_cpu_usage_seconds_total:sum_rate
      - kube_namespace_labels
      - container_memory_rss
      - container_network_receive_bytes_total
      - container_network_transmit_bytes_total
      - container_network_receive_packets_total
      - container_network_transmit_packets_total
      - container_network_receive_packets_dropped_total
      - container_network_transmit_packets_dropped_total
      - container_cpu_usage_seconds_total
    matches:
      - __name__="workqueue_queue_duration_seconds_bucket",job="apiserver"
      - __name__="workqueue_adds_total",job="apiserver"
      - __name__="workqueue_depth",job="apiserver"
      - __name__="go_goroutines",job="apiserver"
      - __name__="process_cpu_seconds_total",job="apiserver"
      - __name__="process_resident_memory_bytes",job="apiserver"
      - __name__="container_memory_cache",container!=""
      - __name__="container_memory_rss",container!=""
      - __name__="container_memory_swap",container!=""
      - __name__="container_memory_working_set_bytes",container_name!=""
    renames:
      mixin_pod_workload: namespace_workload_pod:kube_pod_owner:relabel
      namespace:kube_pod_container_resource_requests_cpu_cores:sum: namespace_cpu:kube_pod_container_resource_requests:sum
      node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
      etcd_mvcc_db_total_size_in_bytes: etcd_debugging_mvcc_db_total_size_in_bytes
    recording_rules:
      - record: apiserver_request_duration_seconds:histogram_quantile_99
        expr: (histogram_quantile(0.99,sum(rate(apiserver_request_latencies_bucket{job=\"apiserver\", verb!=\"WATCH\"}[5m])) by (le)))/1000000
      - record: apiserver_request_duration_seconds:histogram_quantile_99:instance
        expr: (histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{job=\"apiserver\", verb!=\"WATCH\"}[5m])) by (le, verb, instance)))/1000000
      - record: sum:apiserver_request_total:1h
        expr: sum(rate(apiserver_request_count{job=\"apiserver\"}[1h])) by(code, instance)
      - record: sum:apiserver_request_total:5m
        expr: sum(rate(apiserver_request_count{job=\"apiserver\"}[5m])) by(code, instance)
      - record: rpc_rate:grpc_server_handled_total:sum_rate
        expr: sum(rate(grpc_server_handled_total{job=\"etcd\",grpc_type=\"unary\",grpc_code!=\"OK\"}[5m]))
      - record: active_streams_watch:grpc_server_handled_total:sum
        expr: sum(grpc_server_started_total{job=\"etcd\",grpc_service=\"etcdserverpb.Watch\",grpc_type=\"bidi_stream\"}) - sum(grpc_server_handled_total{job=\"etcd\",grpc_service=\"etcdserverpb.Watch\",grpc_type=\"bidi_stream\"})
      - record: active_streams_lease:grpc_server_handled_total:sum
        expr: sum(grpc_server_started_total{job=\"etcd\",grpc_service=\"etcdserverpb.Lease\",grpc_type=\"bidi_stream\"}) - sum(grpc_server_handled_total{job=\"etcd\",grpc_service=\"etcdserverpb.Lease\",grpc_type=\"bidi_stream\"})
      - record: cluster:kube_pod_container_resource_requests:cpu:sum
        expr: sum(sum(sum(kube_pod_container_resource_requests_cpu_cores) by (pod,namespace,container) * on(pod,namespace) group_left(phase) max(kube_pod_status_phase{phase=~\"Running|Pending|Unknown\"} >0) by (pod,namespace,phase)) by (pod,namespace,phase))
      - record: cluster:kube_pod_container_resource_requests:memory:sum
        expr: sum(sum(sum(kube_pod_container_resource_requests_memory_bytes) by (pod,namespace,container) * on(pod,namespace) group_left(phase) max(kube_pod_status_phase{phase=~\"Running|Pending|Unknown\"} >0) by (pod,namespace,phase)) by (pod,namespace,phase))
      - record: sli:apiserver_request_duration_seconds:trend:1m
        expr: sum(increase(apiserver_request_latencies_bucket{job=\"apiserver\",service=\"kubernetes\",le=\"1\",verb=~\"POST|PUT|DELETE|PATCH\"}[1m])) / sum(increase(apiserver_request_latencies_count{job=\"apiserver\",service=\"kubernetes\",verb=~\"POST|PUT|DELETE|PATCH\"}[1m]))
      - record: :node_memory_MemAvailable_bytes:sum
        expr: sum(node_memory_MemAvailable_bytes{job=\"node-exporter\"}or(node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"}))
      - record: instance:node_network_receive_bytes_excluding_lo:rate1m
        expr: sum(rate(node_network_receive_bytes_total{job=\"node-exporter\", device!=\"lo\"}[1m])) without(device)
      - record: instance:node_network_transmit_bytes_excluding_lo:rate1m
        expr: sum(rate(node_network_transmit_bytes_total{job=\"node-exporter\", device!=\"lo\"}[1m])) without(device)
      - record: instance:node_network_receive_drop_excluding_lo:rate1m
        expr: sum(rate(node_network_receive_drop_total{job=\"node-exporter\", device!=\"lo\"}[1m])) without(device)
      - record: instance:node_network_transmit_drop_excluding_lo:rate1m
        expr: sum(rate(node_network_transmit_drop_total{job=\"node-exporter\", device!=\"lo\"}[1m])) without(device)

  uwl_metrics_list.yaml: |
    names:
      - ALERTS
      - etcd_debugging_mvcc_db_total_size_in_bytes
      - etcd_mvcc_db_total_size_in_bytes
      - etcd_debugging_snap_save_total_duration_seconds_sum
      - etcd_disk_backend_commit_duration_seconds_bucket
      - etcd_disk_backend_commit_duration_seconds_sum
      - etcd_disk_wal_fsync_duration_seconds_bucket
      - etcd_disk_wal_fsync_duration_seconds_sum
      - etcd_object_counts
      - etcd_network_client_grpc_received_bytes_total
      - etcd_network_client_grpc_sent_bytes_total
      - etcd_network_peer_received_bytes_total
      - etcd_network_peer_sent_bytes_total
      - etcd_server_client_requests_total
      - etcd_server_has_leader
      - etcd_server_health_failures
      - etcd_server_leader_changes_seen_total
      - etcd_server_proposals_failed_total
      - etcd_server_proposals_pending
      - etcd_server_proposals_committed_total
      - etcd_server_proposals_applied_total
      - etcd_server_quota_backend_bytes
      - up
    matches: []
    renames:
      etcd_mvcc_db_total_size_in_bytes: etcd_debugging_mvcc_db_total_size_in_bytes
    rules: []
    recording_rules:
      - record: apiserver_request_duration_seconds:histogram_quantile_99
        expr: histogram_quantile(0.99,sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\", verb!=\"WATCH\", _id!=\"\"}[5m])) by (le, _id))
      - record: sum:apiserver_request_total:1h
        expr: sum(rate(apiserver_request_total{job=\"apiserver\", _id!=\"\"}[1h])) by(code, instance, _id)
    collect_rules: []